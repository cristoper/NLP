{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropbox = os.path.expanduser(\"~/Dropbox/\")\n",
    "train_path = dropbox + \"NLP Readings/hw 1/POS-training.txt\"\n",
    "test_path= dropbox + \"/NLP Readings/hw 1/POS-test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_tag_from_file(filename):\n",
    "    \"\"\"\n",
    "    filename: the name of the file containing tab-separated words and tags, one per line:\n",
    "        #   word    TAG\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                # skip blank lines\n",
    "                continue\n",
    "            _, word, tag = line.split(\"\\t\")\n",
    "            yield (word, tag)\n",
    "\n",
    "def create_dict(filename):\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping each word to its most frequent tag.\n",
    "    \n",
    "    Args:\n",
    "        filename: the name of the file containing tab-separated words and tags, one per line:\n",
    "        #   word    TAG\n",
    "    \"\"\"\n",
    "    # count all (word, tag) pairs\n",
    "    count_dict = defaultdict(dict)\n",
    "    for word, tag in word_tag_from_file(filename):\n",
    "        # build dictionary of form:\n",
    "        # {'word': {'TAG': count, 'TAG2': count, ...},\n",
    "        # {'word2: {'TAG': count}, ...}}\n",
    "        try:\n",
    "            count_dict[word][tag] += 1\n",
    "        except KeyError:\n",
    "            # this is the first time we've seen this (word, tag) pair\n",
    "            count_dict[word][tag] = 1\n",
    "\n",
    "    print(count_dict[\"'s\"])\n",
    "    # keep only the most frequent tag for each word\n",
    "    # After this, all keys and values are strings\n",
    "    for word, tags in count_dict.items():\n",
    "        if isinstance(tags, dict):\n",
    "            count_dict[word] = max(tags, key=lambda tag: tags[tag])\n",
    "    \n",
    "    # get the most frequent tag and use it for unknown words\n",
    "    tag_counts = Counter(count_dict.values())\n",
    "    top_tag = max(tag_counts, key=lambda c: tag_counts[c])\n",
    "    count_dict['UNK'] = top_tag\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_tag (filename, lookup):\n",
    "    \"\"\"\n",
    "    Assigns a predicted tag and computes accuracy\n",
    "    Arguments:\n",
    "        filename: the name of the file containing tab-separated words and tags, one per line:\n",
    "        #   word    TAG\n",
    "        lookup: dictionary of the tag associated with the word (both strings): {word: tag}\n",
    "        \n",
    "    returns:\n",
    "        accuracy: the accuracy of the file\n",
    "    \"\"\"\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    unk = lookup['UNK']\n",
    "    for word, tag in word_tag_from_file(filename):\n",
    "        # why did amy skip '.'?\n",
    "        if word == \".\":\n",
    "            continue\n",
    "        predicted_tag = lookup.get(word, unk)\n",
    "        if predicted_tag == tag:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "    return correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933558159750169"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tag(train_path, look_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wrong)\n",
    "look_up[\"'s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447830793571488"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tag(test_path,look_up)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
