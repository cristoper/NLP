{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "return \n",
    "- training data\n",
    "- intermediate data\n",
    "- test data\n",
    "- sentence to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/POS-training.txt\",'\\t', \n",
    "                      header=None, \n",
    "                      skip_blank_lines=False, \n",
    "                      keep_default_na = False,\n",
    "                      names = ['word_Num', 'word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sent = ['the', 'dog', 'ate', 'the', 'food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fixed Vocabulary and Tag Lists\n",
    "return \n",
    "- tag list\n",
    "- vocabuarly list called vocabulary\n",
    "- events list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = ['CC', 'CD',\n",
    "        'DT',\n",
    "        'EX',\n",
    "        'FW',\n",
    "        'IN', \n",
    "        'JJ', 'JJR', 'JJS',\n",
    "        'LS', \n",
    "        'MD',\n",
    "        'NN', 'NNS', 'NNP', 'NNPS',\n",
    "        'PDT', 'POS', 'PRP', 'PRP$',\n",
    "        'RB', 'RBR', 'RBS', 'RP',\n",
    "        'SYM', \n",
    "        'TO', \n",
    "        'UH', \n",
    "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "        'WDT', 'WP', 'WP$', 'WRB', \n",
    "        '$', '#', '\"', '(', ')', ',', '.', ':'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_dict(data, ngrams = \"tag_word\"):\n",
    "    \"\"\"\n",
    "    Creates dict of ngrams (key) and count (value). \n",
    "    \n",
    "    Arguments: \n",
    "        data: DataFrame with 'tag' and 'word' colum\n",
    "        negrams: denote type of ngram (unigram or bigram) and if want words or tags: word_word or tag_word\n",
    "    Returns:\n",
    "        A dict where key is either a unigram or a bigram tuple, and value is the count of the ngrams\n",
    "    \"\"\"\n",
    "    if ngrams == \"tag_tag\":     \n",
    "        col_1 = data['tag']\n",
    "        col_2 = col_1[1:col_1.shape[0]]\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "        ngram_count[('', col_1[0])] += 1\n",
    "\n",
    "    \n",
    "    if ngrams == \"tag_word\": # not really bi-grams, just getting count of tag,word\n",
    "        col_1 = data['word']\n",
    "        col_2 = data['tag']\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "            \n",
    "    if ngrams == 'tag': \n",
    "        ngram_count = dict(Counter(data.tag))      \n",
    "        \n",
    "    if ngrams == 'word': \n",
    "        ngram_count = dict(Counter(data.word))\n",
    "            \n",
    "    return ngram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fixed vocabulary; identify which words will be considered UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words that we will call unknowns, and replace these instances in the dataframe\n",
    "unigrams = ngram_dict(train, \"word\")\n",
    "unknowns = { key:value for key, value in unigrams.items() if value < 2 }\n",
    "unknowns = unknowns.fromkeys(unknowns, 'UNK')\n",
    "# replace words that appear less than three times with UNK in training data\n",
    "train['word'] = train['word'].replace(unknowns)\n",
    "#get list of vocabulary \n",
    "vocab = ngram_dict(train, \"word\")\n",
    "vocabulary = list(vocab.keys())\n",
    "vocabulary.remove('') # remove spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return index of the words in the new sentece from the fixed vocabuarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "dog\n",
      "ate\n",
      "the\n",
      "food\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[561, 675, 675, 561, 617]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = []\n",
    "for word in new_sent:\n",
    "    print(word)\n",
    "    try: \n",
    "        events.append(vocabulary.index(word))\n",
    "    except: events.append(vocabulary.index('UNK'))\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Transition and Observation Matrices\n",
    "return\n",
    "-tran matrix\n",
    "-observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_transition_matrix (tags, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the transition matrix (len(tags)+1 x len(tags))\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        bigram_counts: count of bigrams of POS tags in training data (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: 45 x 44 matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    transition = [] # list of transition probabilities \n",
    "    \n",
    "    # first compute the starting probabilities \n",
    "\n",
    "    for x in tags: \n",
    "            pair = ('',x) # here the period denotes the start of a sentence. Not very confident about this\n",
    "            denominator = unigram_counts[''] + len(tags)\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                 numerator = 1\n",
    "            transition.append(numerator / denominator)\n",
    "\n",
    "\n",
    "    # then compute everything else \n",
    "    \n",
    "    for x in tags:\n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try:\n",
    "                denominator = unigram_counts[x] + len(tags)\n",
    "            except: \n",
    "                denominator = len(tags)\n",
    "            try: \n",
    "                numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                numerator = 1 \n",
    "            transition.append(numerator / denominator)\n",
    "   \n",
    "    \n",
    "    transition = np.array(transition)\n",
    "    tran_matrix = transition.reshape(len(tags)+1, len(tags))\n",
    "    \n",
    "    return tran_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 14900,\n",
       " '.': 14901,\n",
       " ':': 5,\n",
       " 'CC': 2008,\n",
       " 'CD': 3951,\n",
       " 'DT': 8063,\n",
       " 'EX': 432,\n",
       " 'FW': 355,\n",
       " 'HYPH': 539,\n",
       " 'IN': 12696,\n",
       " 'JJ': 7363,\n",
       " 'JJR': 1508,\n",
       " 'JJS': 321,\n",
       " 'LS': 9,\n",
       " 'MD': 4717,\n",
       " 'NN': 21147,\n",
       " 'NNP': 686,\n",
       " 'NNS': 5570,\n",
       " 'PDT': 116,\n",
       " 'POS': 578,\n",
       " 'PRP': 12334,\n",
       " 'PRP$': 272,\n",
       " 'RB': 5629,\n",
       " 'RBR': 409,\n",
       " 'RBS': 22,\n",
       " 'RP': 941,\n",
       " 'TO': 4790,\n",
       " 'UH': 3964,\n",
       " 'VB': 13727,\n",
       " 'VBD': 527,\n",
       " 'VBG': 946,\n",
       " 'VBN': 391,\n",
       " 'VBP': 5522,\n",
       " 'VBZ': 2305,\n",
       " 'WDT': 581,\n",
       " 'WP': 748,\n",
       " 'WRB': 1072}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00006692,  1.        ,  1.        ,  0.99666954,  1.        ,\n",
       "        0.96741855,  1.        ,  0.99540975,  1.        ,  1.        ,\n",
       "        0.83018868,  1.        ,  0.98220943,  0.99804061,  0.97260274,\n",
       "        1.        ,  1.        ,  1.        ,  0.99967685,  1.        ,\n",
       "        0.99453552,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.99934645,  1.        ,  1.        ,\n",
       "        0.9908046 ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.00294413,  1.        ])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tag_counts = ngram_dict(train, \"tag_tag\")\n",
    "unigram_tag_counts = ngram_dict(train, \"tag\")\n",
    "transitions = compute_transition_matrix (tags, bigram_tag_counts, unigram_tag_counts)\n",
    "np.sum(transitions, axis = 1) # confirm that most rows sum closely to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036737152034261242"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to=tags.index('MD')\n",
    "transitions[0][to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_observation_matrix (tags, vocabulary, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the observation matrix (tags, vocabulary)\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        vocabulary: words that appear in the training set. Any words that appear less than 2 times = UNK\n",
    "        bigram_counts: count of bigrams of (tag, word) (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: len(tags) x len(vocabulary) matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    observations = [] # list of observation likelihoods\n",
    "    for x in vocabulary: \n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try: \n",
    "                denominator = unigram_counts[y] + len(vocabulary)\n",
    "            except: \n",
    "                 denominator = len(vocabulary)\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair] + 1\n",
    "            except: numerator = 1\n",
    "            observations.append(numerator / denominator)\n",
    "            \n",
    "    observations = np.array(observations)\n",
    "    obs_matrix = observations.reshape(len(tags),len(vocabulary))  \n",
    "    return obs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigram_counts = ngram_dict(train, \"tag_word\")\n",
    "unigram_counts = ngram_dict(train, \"tag\")\n",
    "observations = compute_observation_matrix(tags, vocabulary, bigram_counts, unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01938284,  0.75058859,  0.7485761 ,  0.93363031,  0.89182588,\n",
       "        1.02646177,  1.23411722,  0.67948352,  1.02099207,  1.32632875,\n",
       "        0.99593978,  0.88591532,  0.84509642,  0.75465785,  0.97762593,\n",
       "        0.81547447,  0.90293853,  0.63904234,  0.84768789,  0.7672299 ,\n",
       "        0.81892776,  1.48828228,  1.18169853,  1.03135227,  1.0514001 ,\n",
       "        1.08289383,  0.77321037,  1.40256496,  0.85712965,  0.84533262,\n",
       "        0.85360911,  0.72061329,  1.01323272,  0.64480034,  0.70196164,\n",
       "        1.6792799 ,  1.67764686,  1.78974268,  0.76553816,  0.90918076,\n",
       "        1.40285531,  1.35588952,  0.92210252,  0.96775936])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(observations, axis = 1) # confirm that most rows sum closely to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm\n",
    "return\n",
    "-predicted POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi (transition, observations, events):\n",
    "    \"\"\" Computes sequnce of hidden states, given observed events.\n",
    "    Arguments: \n",
    "        transition: transition matrix with start probabilites as first row\n",
    "        observations: observation liklihood matrix, with states as rows, and vocabulary as columns\n",
    "        events: sequence of observed events\n",
    "        \n",
    "    Returns: \n",
    "        generator, which yields the states\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition.shape[1]\n",
    "    n_events = len(events)\n",
    "    v = np.zeros((n_states, n_events))\n",
    "    bp = v.copy()\n",
    "    \n",
    "    # initialization step\n",
    "    for s in range(n_states):\n",
    "        v[s,0] = tran[0,s] * observations[s, events[0]-1]\n",
    "\n",
    "    # induction step\n",
    "    for t in range (1, n_events):\n",
    "        for s in range(n_states):\n",
    "            tmp = []\n",
    "            for s_prime in range (n_states): \n",
    "                prev_t = v[s_prime, t-1]\n",
    "                tran_s_prime_to_s = tran[s_prime + 1, s]\n",
    "                obser_s_given_t = observations[s, events[t]-1]\n",
    "                tmp.append(prev_t * tran_s_prime_to_s *obser_s_given_t) # still need to changet thos to adding logs\n",
    "            # now that all interim probabilities have been computed for given state, get max\n",
    "            # and also store the index of the argmax\n",
    "            v[s,t] = max(tmp)\n",
    "            bp[s,t] = np.argmax(tmp)\n",
    "\n",
    "    # termination step\n",
    "    q = np.argmax(v[:, n_events-1]) # want to get the argmax of the final time -- it will return a state index\n",
    "\n",
    "    # back reference step \n",
    "    for i in reversed(range(n_events)):\n",
    "        yield q\n",
    "        q = int(bp[q,i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(viterbi_gen, names_events):\n",
    "    \"\"\" translate viterbi generater into a sequence of state anme\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for state in viterbi_gen:\n",
    "        name = names_events[state]\n",
    "        sequence.insert(0, name)\n",
    "        \n",
    "    return(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = viterbi(transitions, observations, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP', 'MD', 'VB', 'TO', 'VB']"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence(tagger, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do list\n",
    "1. make sure I am adding the log of the probabilities, instead of multiplying probabilties in viterbi (and that it still works)\n",
    "2. TONIGHT: write function to print tags from get_sequence into .txt file in a format that can then read in and evaluated by eval.py\n",
    "3. place code in final .py file (make it pretty if have time)\n",
    "4. upon working code, evaluate both training and intermediate test data files\n",
    "5. recompute transition and observation tables with entire training set\n",
    "6. MONDAY NIGHT: wait for final test set to run through system\n",
    "7. MONDAY NIGHT: write up report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
